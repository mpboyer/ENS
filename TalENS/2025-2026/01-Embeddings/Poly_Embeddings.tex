\documentclass[info, math]{mpb-cours}

\title{Modélisation de Données Complexes}
\author{Matthieu Boyer}
\date{Cours TalENS 1 2025-2026}

\begin{document}
\bettertitle

Ce polycopié ne doit pas être vu comme un remplacement au cours, mais simplement comme un résumé du contenu qui permet d'être sûr de ne rien manquer.
On ne parlera ici des espaces vectoriels que dans un cadre très restreint adapté à ce qu'on va vouloir en
faire.
En particulier, on ne fera pas d'algèbre linéaire, on ne parlera pas de base quelconque ni d'isomorphismes.
Je vous renvoie au cours donné avec Clément Allard l'an dernier pour plus de détails et des preuves.

\section{Calcul vectoriel}
\subsection{Espaces euclidiens}
Autour de 300 avant notre ère, Euclide, mathématicien grec écrit les \ul{Éléments}, un traité de géométrie
qui construit le monde autour de 5 axiomes (postulats indémontrables par les autres),
sous forme de constructions géométriques réalisables:
\begin{enumerate}
	\item On peut tracer une ligne droite de tout point à tout autre point;
	\item On peut étendre une ligne droite finie continuement en une ligne droite;
	\item On peut décrire un cercle de tout centre avec tout rayon;
	\item On suppose que tous les angles droits sont égaux;
	\item On suppose qu'étant donnée une droite et un point n'appartenant pas à la droite, il y a exactement une droite qui passe par ce point et qui ne croise pas la droite de départ.
\end{enumerate}

Les espaces vectoriels euclidiens sont une manière algébrique de décrire un espace vérifiant les axiomes
d'Euclide.
\begin{definition}
	Un espace vectoriel (réel) est un ensemble $E$ de points (appelés vecteurs) munis d'une addition interne associative commutative unitale et d'une multiplication externe distributive sur l'addition.
\end{definition}
Autrement dit, dans l'espace $E$:
\begin{itemize}
	\item Il existe un vecteur origine ou vecteur nul, dénoté $0_{E}$ ou simplement $0$;
	\item L'addition de deux vecteurs se fait par la relation de Chasles\footnote{Qui est un théorème de géométrie affine et non vectorielle.};
	\item La multiplication par $\lambda \in \R$ d'un vecteur $x$ est l'agrandissement/le rétrécissement de $x$ par un facteur $\lambda$.
\end{itemize}
Il n'y a pas de "points" dans $E$, mais on peut les construire en voyant un point $P$ comme le vecteur partant de $0$ et allant jusqu'à $P$.

\begin{remarque}
	Il est important de noter que cette définition ne présume rien sur le nombre de vecteurs,
	ni sur ce qu'ils représentent: pour tout ensemble $X$, l'ensemble des fonctions de $X$ dans $\R$
	est un espace vectoriel (muni de l'addition point à point et de la multiplication du résultat).
	On va beaucoup, dans la suite, faire appel à l'intuition géométrique des espaces vectoriels,
	mais il nous est utile d'avoir une définition très générale.
\end{remarque}

\begin{definition}
	Une base d'un espace vectoriel $E$ est une famille $(e_{i})_{i\in I}$ d'éléments de $E$ telle que tout
	vecteur $x$ de $E$ s'écrit de manière unique comme une somme finie de $e_{i}$.
	On dit que $E$ est de dimension finie si $E$ possède une base finie.
\end{definition}

Tous les espaces vectoriels possèdent une base, si l'on accepte l'axiome du choix (ou plutôt le
lemme de Zorn, équivalent).
Tous les espaces vectoriels ne possèdent pas de base finie, comme c'est par exemple le cas de l'espace des fonctions.
Toutes les bases d'un espace vectoriel ont même cardinal, appelé dimension de l'espace.

\begin{definition}
	L'espace engendré par une famille $(e_{i})$ est l'ensemble $\Vect(e_{i})$ des combinaisons linéaires des $(e_{i})$.
\end{definition}
\begin{remarque}
	Être une base, ce n'est pas juste engendrer $E$ (dans ce cas, la famille est génératrice), c'est engendrer $E$ sans avoir de degré de liberté superflu.
	Cela revient à dire qu'il n'existe pas d'indice $k \in I$ tel que $e_{k} \in \Vect(e_{i})_{i \in I \setminus \{k\}}$, et dans ce cas là, on dit que la famille est libre.
	Dans la suite, on ne considèrera que des familles libres (sauf mention contraire).
\end{remarque}

L'espace engendré par un vecteur est une droite, celui engendré par 2 vecteurs est un plan et celui engendré
par $n - 1$ vecteurs dans un espace de dimension $n$ est appelé un hyperplan.

\begin{definition}
	Une application $\phi: E \to F$ est dite linéaire si pour tout $x, y \in E$, pour tout $\lambda \in \R$: $\phi(\lambda x + y) = \lambda \phi(x) + \phi(y)$.
	Une application $\psi: E \times F \to G$ est dite bilinéaire si pour tout $x \in E$ sa restriction à $F$ est linéaire (et de même pour tout $y \in F$).
\end{definition}

\begin{definition}
	Un produit scalaire sur $E$ est une application bilinéaire symmétrique définie positive de $E \times E$ dans $\R$.
	Sa norme engendrée est l'application $\norm{\cdot}: x \mapsto \sqrt{\scalar{x, x}}$
\end{definition}
Symmétrique signifie que $\scalar{x, y} = \scalar{y, x}$, positive signifie que $\scalar{x, x} \geq 0$ et
définie signifie que $\scalar{x, x} = 0 \Rightarrow x = 0$.

\begin{definition}
	Deux vecteurs sont orthogonaux si leur produit scalaire est nul.
	Une base est orthonormale si tous ses vecteurs sont orthogonaux deux à deux et sont de norme 1.
\end{definition}

Tout espace vectoriel admet une base orthonormale.

\begin{proposition}
	Dans le cas où $E$ admet une base orthonormée $(e_{i})$, si $x = \sum x_{i}e_{i}$ et $y = \sum y_{i} e_{i}$ alors:
	\begin{equation*}
		\scalar{x, y} = \sum x_{i}y_{i}
	\end{equation*}
\end{proposition}

Toutes les définitions ci-dessus sont une extension simple de ce qui a été vu en cours pour le produit
scalaire en dimension $2$ et $3$.

\begin{thm}[Pythagore]
	Si la famille des $u_{i}$ est orthogonale:
	\begin{equation*}
		\norm{\sum u_{i}}^{2} = \sum \norm{u_{i}}^{2}
	\end{equation*}
\end{thm}

Lorsque notre espace vectoriel euclidien est de dimension finie, on a le résultat suivant:
\begin{definition}
	Si $E$ est un espace euclidien de dimension $d$, alors $E$ est isomorphe à $\R^{d}$.
	Autrement dit, tout calcul fait dans $E$ est équivalent à un calcul fait dans $\R^{d}$.
\end{definition}
Ce résutlat est important car dans la suite, nous ne travaillerons que sur des espaces dont
nous pouvons connaître la structure: ou bien ils ont des propriétés particulières (espaces de fonctions,
de suites, etc...) ou bien ils sont $\R^{d}$ pour un certain $d$.
Nous pouvons donc nous limiter à définir ce que nous voulons sur $\R^{d}$, en offrant possiblement un sens
dérivé de l'espace de départ à nos applications sur $\R^{d}$.

Il existe une base dite canonique de $\R^{d}$, notée $e_{1}, \ldots, e_{d}$ et orthonormale.

\subsection{Applications sur les espaces euclidiens}
Dans la suite, toutes les fonctions sur lesquelles nous travaillerons seront en dimension finie.
On va chercher à généraliser rapidement la définition de dérivée:
\begin{definition}
	La dérivée directionnelle de $f: \R^{n} \to \R$ selon $v \in \R^{n}$ en $x \in \R^{n}$ est donnée par:
	\begin{equation*}
		D_{v}(f)(x) = \lim_{t \to 0} \frac{f(x + tv) - f(x)}{t} \in \R
	\end{equation*}
\end{definition}
\begin{definition}
	Le gradient de $f$ en $x$ est le vecteur $\nabla(f)(x)$ des dérivées directionnelles de $f$ en $x$ selon
	chacun des $e_{i}$:
	\begin{equation*}
		\nabla(f)(x) =
		\begin{pmatrix}
			D_{e_{1}}(f)(x) \\
			\vdots          \\
			D_{e_{d}}(f)(x)
		\end{pmatrix} =
		\begin{pmatrix}
			\frac{\partial f}{\partial x_{1}}(x) \\
			\vdots                               \\
			\frac{\partial f}{\partial x_{d}}(x)
		\end{pmatrix}
	\end{equation*}
	C'est un vecteur de $\R^{d}$.
\end{definition}

Le gradient est le vecteur qui indique la direction de plus forte progression de la fonction.

\begin{proposition}
	L'opérateur $\nabla(\cdot) (x)$ est linéaire.
\end{proposition}

Cette proposition indique qu'en effet, l'opérateur $\nabla$ désigne bien une sorte de dérivée.
D'ailleurs, lorsque $d = 1$, c'est exactement la dérivée dont vous avez l'habitude.
Voici juste un exemple utile:
\begin{equation*}
	\nabla\left(\norm{\cdot}^{2}\right)(x) = 2x \quad \nabla(\norm{\cdot})(x) = \nabla\left(\norm{\cdot}\right)(x) \times \frac{1}{2\norm{x}} = \frac{x}{\norm{x}}
\end{equation*}
Attention, le gradient n'existe pas toujours, comme le montre l'exemple ci-dessus, qui n'a pas de gradient
en $0$.
L'expression pour le gradient de la norme découle de la règle de la chaîne: $\frac{\partial f\circ g}{\partial x} = \frac{\partial g}{\partial x} \times f'(g(x))$ où $f: \R \to \R$ et $g: \R^{d} \to \R$.
Il y a une forme plus générale de la règle, mais elle n'est pas nécessaire pour comprendre la suite.

\begin{definition}
	La Hessienne de $f$ en $x$ est en quelque sorte la dérivée seconde de $f$, c'est la matrice (vecteur $2$-dimensionnel ou tableau):
	\begin{equation*}
		H(f)(x) =
		\begin{pmatrix}
			\frac{\partial f}{\partial x_{1}\partial x_{1}}(x) & \cdots & \frac{\partial{f}}{\partial x_{d}\partial x_{1}}(x)  \\
			\vdots                                             & \ddots & \vdots                                               \\
			\frac{\partial f}{\partial x_{1}\partial x_{d}}(x) & \cdots & \frac{\partial{f}}{\partial{x_{d}}\partial x_{d}}(x) \\
		\end{pmatrix}
		= \left(\frac{\partial f}{\partial x_{i}\partial x_{j}} (x)\right)_{i, j}
	\end{equation*}
	où $\frac{\partial f}{\partial x_{i}\partial x_{j}}$ désigne la dérivée selon $x_{j}$ de la fonction dérivée selon $x_{i}$ de $f$.
\end{definition}

\begin{thm}
	Comme sur $\R$, un point $x$ est un extremum local de $f$ si $\nabla(f)(x) = 0$.
	De même, un point $x$ est un minimum local si $H(f)(x)$ est définie positve.
\end{thm}


\section{Plongements euclidiens}
On s'intéresse maintenant au problème de la construction d'ensembles de données sur lesquelles travailler.

\subsection{Plongements}
Il n'y a pas de définition formelle de plongement qui n'est pas spécifiique à un type de plongement.
En général cependant, un plongement d'une structure $A$ dans une structure $B$ est une forme de morphisme
injectif (ou monomorphisme).

\begin{definition}
	Un plongement $\iota$ de $X$ dans un espace euclidien $\R^{F}$ dit espace de représentation ou
	espace de caractéristiques (feature space), est une application injective qui vérifie
	l'heuristique suivante:
	Étant donnés deux objets $a, b$ de $X$, les deux vecteurs $\iota(a), \iota(b)$ de l'espace euclidien
	vérifient, pour toute fonction qu'on souhaite étudier:
	\begin{equation*}
		f(\iota(a), \iota(b)) = \Theta(f(a, b))
	\end{equation*}
	où $\Theta$ désigne la notation de Landau pour l'encadrement à constante près ($Cf(a, b) \leq f(\iota(a), \iota(b)) \leq C'f(a, b)$)
\end{definition}

\begin{remarque}
	C'est une forme d'application continue injective pour la topologie engendrée par tiré en arrière des métriques que nous allons étudier sur l'espace de représentation.
	Si cette phrase ne veut rien dire pour vous, c'est normal pour l'instant,
	nous reviendrons sur ça lors d'un cours ultérieur.
\end{remarque}

L'acte de plonger ses données de départ dans un espace euclidien n'est pas à prendre à la légère,
c'est lui qui engendre la plupart des propriétés que nous pouvons étudier:
D'une part on va chercher à reduire au maximum la dimension $F$ de l'espace de caractéristiques pour
limiter les besoins en mémoire et en temps des calculs;
D'autre part, si le plongement n'est pas fidèle aux données de départ, et n'est pas réellement injectif (par exemple si on réduit trop la dimension en ne se basant que sur
un échantillon fini d'un ensemble réellement infini de données).

\subsection{Zoologie de métriques}
Dans cette partie on va regarder rapidement quelques métriques intéressantes sur des ensembles de points
dans un espace vectoriel $E$.
Il faut toujours se souvenir que définir une métrique sur un espace vectoriel doit se faire en ayant en tête
l'ensemble qui a été plongé dans l'espace vectoriel et ce que la métrique peut nous apprendre sur l'espace.

\subsubsection{Similarité angulaire}
On va chercher à mesurer l'angle entre deux droites engendrées par $u$ et $v$.
\begin{proposition}[Inégalité de Cauchy-Schwarz]
	Si $u, v$ sont des éléments d'un espace euclidien alors:
	\begin{equation*}
		\abs{\scalar{u, v}} \leq \norm{u}\norm{v}
	\end{equation*}
	avec égalité si et seulement si $u = \lambda v$ avec $\lambda \geq 0$.
\end{proposition}

En particulier cela signifie que
\begin{equation*}
	\frac{\scalar{u, v}}{\norm{u}\norm{v}} \in [-1, 1]
\end{equation*}
et donc que $\arccos\left(\frac{\scalar{u, v}}{\norm{u}\norm{v}}\right)$ est bien une mesure d'angle entre les deux droites.

\subsubsection{Norme L-p}
\begin{definition}
	Si $p \geq 1$ et $x \in E$.
	\begin{equation*}
		\norm{x}_{p} = \sqrt[p]{\sum x_{i}^{p}}
	\end{equation*}
\end{definition}

Pour tout $p$, on obtient une distance entre nos points, par $d_{p}(x, y) = \norm{x - y}_{p}$.
Pour $p = \infty$ on pose $\ninf{x} = \max{\abs{x_{i}}}$.

\subsubsection{Représentation fréquentielle}
En divisant $x$ par la somme des $x_{i}$, on construit une distribution de probabilité à partir de $x$.
Ceci nous permet d'utiliser tout un tas de métriques sur l'ensemble des distributions de probabilité,
notamment les distances de Wasserstein, la divergence de Kullback-Leibler ou la métrique de Fisher-Rao.

\section{Algorithmes sur les immersions}
\subsection{PCA}
Le but de l'algorithme PCA est de trouver une meilleure base de l'espace pour représenter nos données.
Pour ce faire on va construire une base dont chacune des composantes augmente le plus possible l'espace
précédent.
Ceci nous permet d'ailleurs de construire un espace de plongement de plus faible dimension, au coût de la
signification des caractéristiques.
% Projection
\begin{algorithmic}
	\Procedure{PCA}{n, X}
	\For{$i = 1 \to n$}
	\State{$u_{i} \gets \argmin_{u \in \Vect(u_{j})_{j < i}^{\bot}} \sum_{x \in X}\scalar{x, u} $}
	\EndFor
	\EndProcedure
\end{algorithmic}

PCA est facile à implémenter et préserve la variance globale des données, en restant linéaire et peu
coûteux mais fonctionne assez mal dans le cas où les données ne sont pas du tout linéaire.

\subsection{t-SNE}
Pour t-SNE, l'algorithme est plus complexe et moins interprétable, mais préserve la similitude locale
de l'ensemble.
On plonge les données dans un ensemble de probabilités conditionnels représentant des similitudes pour une distribution gaussienne.
On peut ensuite mesurer la différence entre une distribution apprise (en $2$ ou $3$ dimensions) et cette distribution, par la divergence de Kullback-Leibler.
La divergence de Kullback-Leibler est simplement une mesure de la probabilité que deux distributions de
probabilité soit fortement différentes.

\end{document}
