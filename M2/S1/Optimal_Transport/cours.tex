\documentclass[info, math, french]{mpb-cours}

\title{Transport Optimal Computationnel}
\author{D'après Gabriel Peyré}

\DeclareMathOperator{\Wass}{W}

\begin{document}
\bettertitle
\begin{abstract}
	\url{mailto:gabriel.peyre@ens.fr}
	Notes de cours sur \url{https://arxiv.org/abs/2505.06589}
	Syllabus: \url{https://docs.google.com/document/u/0/d/1JlDpcS0tkzX8CSgHlUf13ZHQRWu650EtNLrycT39dxk/mobilebasic}
\end{abstract}

\section*{Introduction}
L'une des motivations principales du cours est de comparer, en apprentissage statistique,
des données sous formes de distributions de probabilités, souvent discrètes (nuages de points).
On peut penser au transport optimal comme de l'apprentissage non-supervisé: comment associer une
distribution de probabilité paramétrique $\alpha_{\theta}$ à une probabilité observée $\beta$ sur un groupe de points.
Dans ce cours, les lettres grecques sont réservées aux distributions de probabilités et les lettres latines
aux points.
Pour ce faire, on va faire une association de densité $\min_{\theta} D(\alpha_{\theta}, \beta)$ qui
prend en compte une métrique $d$.
L'idée étant d'utiliser la métrique $d$ pour définir la métrique $D$, en utilisant la structure
de l'espace sous-jacent pour les données.
Il faut voir le transport optimal comme un mécanisme d'élévation de l'espace des données vers un espace
de probabilité, de sorte que $D = d$ lorsqu'on considère des diracs.


\section{Problème de Monge}
\subsection{Formulation de Monge}
Le problème est a été défini pour le but militaire de construire des murs avec des sacs de sables déplacés
par des soldats, par Gaspard \textsc{Monge}, dans un papier à l'académie des sciences.

Commençons par un exemple: si on part d'un ensemble de $3$ points $x_{1}, \ldots, x_{3}$ et qu'on veut atteindre $y_{1}, \ldots, y_{3}$, quelle est la manière optimale de donner une bijection dont le coût du
déplacement est minimal ?
On cherche $T: x_{i} \to y_{\sigma(i)}$ pour $\sigma \in \mathfrak{S}_{3}$.

\begin{definition}
	Le problème de Monge est le problème d'optimisation suivant:
	\begin{equation}
		M = \min_{\sigma \in \mathfrak{S}_{n}}\sum_{i = 1}^{n} C_{i, \sigma(i)} \tag{Monge}\label{eq:M}
	\end{equation}
	où $C \in \R^{n \times n}$. On dit que $M$ est l'assignation optimale.
\end{definition}

En général, on définit $C_{i, j} = c(x_{i}, y_{j})$, et $c$ sera généralement la distance géodésique sur une variété, ou une puissance de la norme $p$ sur $\R^{d}$.
Il n'y a pas besoin de supposer que $x$ et $y$ appartiennent aux mêmes espaces $\X$ et $\Y$.

Il est clair que le problème de Monge est combinatoirement complexe, puisque $\abs{\mathfrak{S}_{n}} = n!$.
Monge, historiquement, a proposé des liens avec l'optique. L'académie des sciences a proposé un prix a
celui qui réussirait a proposer une solution (entendre de nos jours, algorithme polynomial pour la réponse).
Aujourd'hui, nous avons un algorithme en $\O(n^{3})$, qui est optimal dans le pire cas.

\subsubsection{Zoologie de Sous-Problèmes}
Dans cette section, on va s'intéresser à quelques sous-problèmes spécifiques, dont le résultat est calculable.

\begin{description}
	\item[Cas $1$-D]
	      Ici, on suppose que $\X = \Y = \R$, et que $c(x, y) = h(x - y)$ pour $h$ convexe.
	      En général, on aura $c(x, y) = \abs{x - y}^{p}$ avec $p \geq 1$.
	      Monge avait étudié le cas $p = 1$ qui est de loin le plus difficile.

	      Dans ce cas, la solution est l'arrangement croissant: l'application $T$ définit ci-dessus est croissante.
	      On trie les points de gauche à droite et on assigne le plus petit $x_{i}$ au plus petit $y_{i}$ et ainsi de suite:
	      \begin{thm}
		      Dans le cas $1$-D, si $M$ vérifie:
		      \begin{equation*}
			      x_{i} < x_{j} \Rightarrow y_{M(i)} < y_{M(j)}
		      \end{equation*}
		      alors $M$ est une solution de \eqref{eq:M}.
	      \end{thm}
	      \begin{proof}
		      Si la propriété n'est pas vérifiée, alors il existe $(i, i')$ tels que $(x_{i} - x_{i'})(y_{\sigma(i)} - y_{\sigma(i')}) < 0$ et en composant $\sigma$ par la transposition $\tau_{i, i'}$ on obtient:
		      \begin{equation*}
			      h(x_{i} - y_{\tau(\sigma(i))}) + h(x_{i'} - y_{\tau(\sigma(i'))}) < h(x_{i} - y_{(\sigma(i)}) + h(x_{i'} - y_{\sigma(i')})
		      \end{equation*}
		      par convexité de $h$.
	      \end{proof}
	      \begin{corollaire}
		      Dans ce cas, on a une complexité en $\O(n \log n)$, en utilisant un algorithme de tri.
	      \end{corollaire}
	      \begin{remarque}
		      Dans le cas où $h$ est strictement convexe, tous les arrangements optimaux sont croissants, et donc on a l'unicité de la solution $M$ dans le cas où tous les points sont distincts.
	      \end{remarque}
	      Ce cas intervient notamment dans le cas ou par exemple on veut comparer deux groupes de niveaux,
	      ou l'égalisation en niveaux de gris de deux histogrammes de luminance (balance des blancs).

	      L'algorithme ne se généralise pas en deux dimensions, puisque les trajectoires ne
	      peuvent pas se recouper pour un arrangement optimal (par l'inégalité du parallélogramme)
	      mais que cette propriété n'implique pas l'optimalité de la solution.
	      Pire, il peut exister un nombre exponentiel de solution non-optimales dans ce cas.

	\item[Couplage] Dans le cas où on peut modéliser le problème par un problème de couplage
	      de graphes, on peut utiliser l'algorithme hongrois pour obtenir une solution en $\O(n^{3})$.
\end{description}

\subsection{Formulation de Monge Continue}
Ici, on va s'intéresser à des mesures boréliennes $\alpha \in \M(\X), \beta \in \M(\Y)$,
pour $\X, \Y$ des espaces métriques.
Pour des questions de facilité, on supposera $\X, \Y$ compacts, et pour l'optimisation on les supposera
de plus complets, généralement des parties de $\R^{d}$.
Lorsqu'on aura besoin de retirer l'hypothèse de compacité, on ne demandera de $\X$ et $\Y$
que d'être des espaces polonais.
On suppose de plus que $\alpha$ et $\beta$ sont des mesures de probabilité (positives et de somme $1$).
On notera l'espace de mesures de probabilité $\M_{+}^{1}(\X) = \mP(\X)$.
\begin{thm}[Riesz-Markov]
	Si $\X$ est un espace polonais, et $\mC_{c}(\X)$ est l'ensemble des fonctions continues à support compact sur $\X$ muni de la norme $\norm{\cdot}_{\infty}$:
	\begin{equation*}
		\M(\X) = \left(\mC_{c}(\X)\right)^{\circ}
	\end{equation*}
\end{thm}
\begin{proof}
	On utilise pour identification le produit scalaire: $\scalar{f, \alpha} = \int f \d\alpha$.
\end{proof}
C'est une généralisation du Théorème de Riesz sur un espace de Hilbert.
Quelques exemples:
\begin{description}
	\item[Discrète] $\alpha = \sum a_{i}\delta_{x_{i}}$ où $a \in \Delta_{n}$.
	      On appellera souvent le vecteur $a$ l'histogramme, et le vecteur $(\delta)$ le nuage de points.
	      Par linéarité, on a:
	      \begin{equation*}
		      \int f\d\left[\sum a_{i}\delta_{x_{i}}\right] = \sum a_{i}f(x_{i})
	      \end{equation*}
	\item[Continue/À densité] Si $\alpha = \rho_{\alpha}\beta$ a densité $\rho_{\alpha}$ par rapport à $\beta$:
	      \begin{equation*}
		      \int f(x)\d\alpha(x) = \int f(x) \rho_{\alpha}(x)\d\beta(x)
	      \end{equation*}
	      C'est-à-dire: $\scalar{\cdot, \alpha} = \scalar{\cdot \times \rho_{\alpha}, \beta}$. On notera ceci $\alpha << \beta$.
\end{description}

Le produit scalaire défini ci-dessus induit une norme duale dite norme de variation totale:
\begin{equation*}
	\norm{\alpha}_{\star} = \norm{\alpha}_{TV} = \sup_{\ninf{f} \leq 1}\scalar{f, \alpha}
\end{equation*}

\begin{proposition}
	Pour $\alpha, \beta \in \M(\X)$:
	\begin{equation*}
		\norm{\alpha - \beta}_{TV} = \abs{\alpha - \beta}(\X) = \int_{\X} d(\abs{\alpha -\beta})(x)
	\end{equation*}
\end{proposition}
Quelques exemples:
\begin{description}
	\item[Discrète] Si $\alpha = \sum a_{i}\delta_{x_{i}}$ et $\beta = \sum b_{i}\delta_{x_{i}}$ où $a, b \in \Delta_{n}$ (ici, on suppose que les nuages de points sont égaux, mais pas que les $a_{i}, b_{i}$ sont non nuls):
	      \begin{equation*}
		      \norm{\alpha - \beta}_{TV} = \sum_{i} \abs{x_{i} - y_{i}}
	      \end{equation*}
	\item[Continue/Denses] Si $\alpha = \rho_{\alpha}\mL$ et $\beta = \rho_{\beta}\mL$:
	      \begin{equation*}
		      \norm{\alpha - \beta}_{TV} = \int \abs{\rho_{\alpha} - \rho_{\beta}}\d\mL
	      \end{equation*}
	\item[Mesures Singulières] Si $\alpha = 0$ quand $\beta \neq 0$ et réciproquement, $\norm{\alpha - \beta}_{TV} = 2$
\end{description}

\begin{definition}[Push-Forward]
	Si on a une application de transport $T: \X \to \Y$, on peut définir une application $T_{\sharp}: \P(\X) \to \P(\Y)$ définie par:
	\begin{enumerate}
		\item $T_{\sharp}\delta_{x} = \delta_{T(x)}$;
		\item $T_{\sharp}$ est linéaire.
	\end{enumerate}
	De manière équivalente: $\beta = T_{\sharp}\alpha$ qui vérifie $\beta(B) = \alpha(T^{-1}(B))$.
\end{definition}

\begin{proposition}
	Si $\beta = T_{\sharp}\alpha$, on a:
	\begin{equation*}
		\int g\d\beta = \int g \circ T \d\alpha, \text{i.e. } \E(g(Y)) = \E(g(T(X)))
	\end{equation*}
	pour $Y = T(X)\tilde \beta$ un vecteur aléatoire.
\end{proposition}


On peut alors réécrire le problème de Monge dans le cas continu:
\begin{definition}
	Si $\alpha \in \P(\X), \beta \in \P(\Y)$, et $c$ est une fonction de coût:
	\begin{equation}
		M = \inf_{T : \X \to \Y} \left\{\int c(x, T(x)) \d\alpha(x)\suchthat T_{\sharp}\alpha = \beta\right\}  \tag{Monge}\label{eq:Mc}
	\end{equation}
\end{definition}

\begin{proposition}
	Dans le cas discret: $T_{\sharp}\alpha = \beta \Leftrightarrow \exists \sigma \in \mathfrak{S}_{n}, T(x_{i}) = y_{\sigma(i)}$.
\end{proposition}

\begin{thm}
	Si $\alpha$ a une densité par rapport à $\beta$, alors il existe $T$ telle que $T_{\sharp}\alpha = \beta$.
\end{thm}
\begin{remarque}
	Dans le cas discret, cela revient à dire que $\abs{supp(\alpha)} \geq \abs{supp(\beta)}$.
	Attention, cette application $T$ n'est pas unique~!
\end{remarque}

\begin{thm}[Brenier, 1991]
	Si $\X = \Y = \R^{d}$ et $c(x, y) = \norm{x - y}^{2}$
	(généralisé à une distance géodésique mis à la puissance $1 < p < +\infty$)
	et si $\alpha$ a une densité par rapport à la mesure de Lebesgue
	(ou au moins ne donne pas de mesure aux espaces de dimension $< d$) alors il y a un unique
	transport optimal $T$ solution de \eqref{eq:Mc} et $T$ est caractérisé par $T = \nabla \phi$
	où $\phi$ est convexe (et $\nabla$ est le gradient riemannien).
	Il y a un unique $\nabla \phi$ où $\phi$ est convexe et $(\nabla \phi)_{\sharp}\alpha = \beta$, c'est
	le transport optimal.
\end{thm}
\begin{corollaire}
	Les gradients des fonctions convexes $\mathcal{G}$ sont des transports optimaux.
\end{corollaire}

\begin{proposition}
	Pour $T \in \mathcal{G}$, $\scalar{Tx - Ty, x - y} \geq 0$, les gradients de fonctions convexes sont dans l'ensemble $\mathcal{H}$ des fonctions croissantes.
\end{proposition}

Dans le cas $\X = \Y = \R$, on a même $\mathcal{G} = \mathcal{H}$.
En plus grande dimension, $T(x) = Rx$ où $R$ est une petite rotation n'est pas un gradient mais est croissante.
Il est possible que $\phi$ ne soit pas différentiable, mais on peut montrer que $\phi$ va être différentiable presque partout, et donc nos équations sont à entendre comme des égalités à ensemble de mesure nulle près.
\begin{proposition}
	$T(x) = Ax$ est un gradient si et seulement si $A$ est symétrique. $T \in \mathcal{G}$ si et seulement si $A$ est définie positive.
\end{proposition}


\begin{definition}
	La $2$-distance de Wasserstein est définie par:
	\begin{equation*}
		W_{2}(\alpha, \beta) = \inf_{T_{\sharp}\alpha = \beta} \int \norm{x - Tx}^{2}\d\alpha(x)
	\end{equation*}
\end{definition}

Quelques exemples avec les gaussiennes:
\begin{description}
	\item[Cas 1D] Ici, on suppose $\alpha = \mN(m_{\alpha}, \sigma_{\alpha}^{2})$, $\beta = \mN(m_{\beta}, \sigma_{\beta}^{2})$.
	      En prenant $T(x) = \frac{\sigma_{\beta}}{\sigma_{\alpha}}(x - m_{\alpha}) + m_{\beta}$, on vérifie bien que $T_{\sharp}\alpha = \beta$ et en primitivant $T$, on vérifie bien que c'est le gradient (la dérivée) d'une fonction convexe et c'est donc bien un transport optimal.
	      Le coût du transport est:
	      \begin{equation*}
		      W_{2}^{2}(\alpha, \beta) = \norm{m_{\alpha} - m_{\beta}}^{2} + \norm{\sigma_{\alpha} - \sigma_{\beta}}
	      \end{equation*}
	\item[Cas $d$-Dimensionnel] Ici, $\alpha = \mN(m_{\alpha}, \Sigma_{\alpha})$ et $\beta = \mN(m_{\beta}, \Sigma_{\beta})$ où $\Sigma_{\alpha} = \E((X- m_{\alpha})\transpose{(X-m_{\alpha})})$ est une matrice définie positive.
	      On suppose que $\Sigma_{\alpha} > 0$.
	      On prend alors $T(x) = A(x - m_{\alpha}) + m_{\beta}$ où $A\Sigma_{\alpha}\transpose{A} = \Sigma_{\beta}$ pour trouver le transport optimal.
	      Cette équation (dite de Ricatti) a une solution symétrique définie positive.
	      On a de plus:
	      \begin{equation*}
		      W_{2}^{2}(\alpha, \beta) = \norm{m_{\alpha} - m_{\beta}}^{2} + \B^{2}(\Sigma_{\alpha}, \Sigma_{\beta})
	      \end{equation*}
	      où $\B$ est la distance de Bures définie par:
	      \begin{equation*}
		      \B(A, B) = \Tr(A + B - 2(A^{1/2}BA^{1/2})^{1/2})
	      \end{equation*}
\end{description}

\section{Formulation de Kantorovitch}
\subsection{Définition}
La formulation de Kantorovitch est une relaxation convexe de la formulation de Monge.
Il a obtenu un prix Nobel d'économie pour ceci.
Ici, on se limite au cas discret $\alpha = \sum_{n} \alpha_{i}\delta_{x_{i}}$ et $\beta = \sum_{m} b_{j}\delta_{y_{j}}$.

\begin{definition}
	Un \emph{couplage} ou un \emph{plan} est une matrice $M \in \R_{+}^{n \times m}$ qui représente le coût de transport de $x_{i}$ à $y_{j}$ et telle que:
	\begin{equation*}
		\sum_{j} P_{i, j} = a_{i} \land \sum_{i}P_{i, j} = b_{j}
	\end{equation*}
\end{definition}
\begin{remarque}
	C'est la même notion que celle de couplage en probabilité: un vecteur aléatoire sur l'espace produit.
\end{remarque}

On remarquera que les équations définissant un plan peuvent se mettre sous la forme:
\begin{equation*}
	P\mathds{1}_{m} = a \text{ et } \transpose{P}\mathds{1}_{n} = b
\end{equation*}

On autorise ainsi la division de masse, les problèmes de Kantorovitch devenant des problèmes sur des graphes bipartis.

\begin{definition}
	Le polytope des couplages entre $\alpha$ et $\beta$ est l'ensemble des plans entre $\alpha$ et $\beta$:
	\begin{equation*}
		\mathrm{Couplages}(\alpha, \beta) = \left\{P \in \R_{+}^{n \times m} \suchthat P_{i, j} \geq 0, P\mathds{1}_{m} = a \text{ et } \transpose{P}\mathds{1}_{n} = b\right\}
	\end{equation*}
\end{definition}

Kantorovitch avait fait l'hypothèse très forte que l'économie est linéaire.

\begin{definition}
	Le \emph{problème de Kantorovitch} est le problème d'optimisation suivant:
	\begin{equation}
		P = \argmin_{P} \left\{\scalar{C, P} \suchthat P \in \mathrm{Couplages}(\alpha, \beta)\right\} \tag{Kantorovitch}\label{eq:K}
	\end{equation}
	où $C \in \R^{n \times m}$ est une matrice de coût. On dit que $P$ est le plan optimal.
\end{definition}

C'est un problème de programmation linéaire.
En général la méthode du simplexe n'est pas polynomial, mais il existe un type de simplexes pour lequel elle l'est, et est en $\O\left(\left(n^{3}m + m^{3}n\right)\log\left(mn\right)\right)$

\begin{proposition}
	Il existe toujours une solution, et il existe toujours une solution dite \emph{éparse}, telle que:
	\begin{equation*}
		\abs{\left\{(i, j) \suchthat P_{i, j}\neq 0\right\}} \leq n + m - 1
	\end{equation*}
\end{proposition}
\begin{proof}
	La preuve d'existence vient du fait que l'ensemble des couplages est un compact non vide (car $P = a\transpose{b}$ est un couplage dit \emph{indépendant}).
\end{proof}
Le cas générique est de plus unique, c'est-à-dire que si $C, \alpha, \beta$ n'a pas une unique solution, en ajoutant du bruit on retrouve une solution optimale.

\subsection{Équivalence à Monge}

On s'intéresse ensuite aux matrices de permutation $P_{n}$, dans le cas $n = m$.
On cherche à résoudre le problème non-convexe $\displaystyle\min_{P\in P_{n}} \scalar{C, P}$.
Clairement, si $\B_{n}$ est l'ensemble convexe des matrices bistochastiques (l'ensemble des couplages~!):
\begin{equation*}
	\min_{P \in \B_{n}} \scalar{C, P} \leq \min_{P \in P_{n}}\scalar{C, P}
\end{equation*}

\begin{definition}
	L'ensemble des points extrême d'un convexe $C$ est:
	\begin{equation*}
		\mathrm{Extr}(C) = \left\{P \suchthat \forall \left(Q, R\right) \in C^{2}, P = \frac{Q + R}{2} \Rightarrow Q = R\right\}
	\end{equation*}
\end{definition}

\begin{thm}
	Si $C$ est compact, $\mathrm{Extr}(C) \neq \emptyset$.
\end{thm}

\begin{thm}[Krain-Millman]
	Si $C$ est un compact convexe, alors $C = \mathrm{Hull}\left(\mathrm{Extr}\left(C\right)\right)$.
\end{thm}

\begin{proposition}
	Si $C$ est compact:
	\begin{equation*}
		\mathrm{Extr}(C) \cap \left(\argmin_{p\in C} \scalar{C, P}\right) \neq \emptyset
	\end{equation*}
\end{proposition}
\begin{proof}
	En notant que l'ensemble de droite est convexe et compact, on trouve $\mathrm{Extr}(S) \neq \emptyset$ et que
	$\mathrm{Extr}(S) \subseteq \mathrm{Extr}(C)$.
\end{proof}

\begin{thm}[Birkhoff - von Neumann]
	On a $\mathrm{Extr}(\B_{n}) = P_{n}$
\end{thm}
\begin{proof}
	On montre d'abord $P_n \subset \mathrm{Extr}(\B_n)$.
	Cela découle de $\text{Extr}([0,1]) = \{0,1\}$.
	Si $P \in P_n$, est telle que $P=(Q+R)/2$ avec $Q_{i,j},R_{i,j} \in [0,1]$, puisque $P_{i,j} \in \{0,1\}$ alors nécessairement $Q_{i,j}=R_{i,j} \in \{0,1\}$.

	Montrons maintenant $\text{Extr}(\B_n) \subset P_n$ en montrant que
	$P_n^c \subset \text{Extr}(\B_n)^c$ avec le complémentaire considéré dans $\B_n$.
	Choisir $P \in \B_n \setminus P_n$ revient à choisir $\P = (Q+R)/2$ où $Q,R$ sont des matrices
	bistochastiques distinctes.
	$P$ définit un graphe biparti de taille $2n$.
	Le graphe est composé d'arêtes isolées quand $P_{i,j}=1$ et d'arêtes connectées quand $0 < P_{i,j} <1$.
	Si $i$ est un tel sommet à gauche ($j$ à droite), puisque $\sum_j P_{i,j}=1$, il y a deux arêtes $(i,j_1)$ et $(i,j_2)$ en sortant (de même, $(i_1,j)$ et $(i_2,j)$ entrant en $j$).
	On peut donc toujours extraire un cycle par récurrence de la forme:
	\begin{equation*}
		\left(i_1,j_1,i_2,j_2,\ldots,i_p,j_p\right),
		\quad \text{i.e.}\quad i_{p+1}=i_1.
	\end{equation*}
	On suppose que ce cycle est le plus court de l'ensemble fini de cycle. On a toujours:
	\begin{equation*}
		0 < P_{i_s,j_s}, P_{i_{s+1},j_s} < 1.
	\end{equation*}
	Les $(i_s)_s$ et $(j_s)_s$ sont distincts puisque le cycle est le plus court. On pose:
	\begin{equation*}
		\epsilon = \min_{0 \leq s \leq p} \left\{ P_{i_s,j_s}, P_{j_s,i_{s+1}}, 1-P_{i_s,j_s}, 1-P_{j_s,i_{s+1}} \right\}
	\end{equation*}
	c'est-à-dire $0 < \epsilon < 1$.
	En séparant le graphe en deux ensembles d'arêtes:
	\begin{equation*}
		\A = \left\{(i_s,j_s)\right\}_{s=1}^p
		\quad \text{ et } \quad
		\B = \left\{(j_s,i_{s+1})\right\}_{s=1}^p.
	\end{equation*}
	On pose $Q$ et $R$ telle que:
	\begin{equation*}
		Q_{i,j} =
		\begin{cases}
			P_{i,j}            & \text{si } (i,j) \notin \A \cup \B, \\
			P_{i,j}+\epsilon/2 & \text{si } (i,j) \in \A,            \\
			P_{i,j}-\epsilon/2 & \text{si } (i,j) \in \B,
		\end{cases}
		\quad \text{ et }\quad
		R_{i,j} =
		\begin{cases}
			P_{i,j}            & \text{si } (i,j) \notin \A \cup \B, \\
			P_{i,j}-\epsilon/2 & \text{si } (i,j) \in \A,            \\
			P_{i,j}+\epsilon/2 & \text{si } (i,j) \in \B,
		\end{cases}.
	\end{equation*}
	Par définition d'$\epsilon$, on a $0 \leq Q_{i,j}, R_{i,j} \leq 1$.
	Puisque chaque arête gauche de $\A$ a une arête droite dans $\B$, (et réciproquement) la
	contrainte de somme sur les lignes (et sur les colonnes) est maintenue, donc $Q,R \in \B_n$. Finalement, on trouve: $P=(Q+R)/2$.
\end{proof}

\begin{corollaire}
	Pour $m = n$ et $a = b = \mathds{1}_{n}$, il existe une solution optimale pour le problème \ref{eq:K}, qui est une matrice de permutation associée à une permutation optimale pour le problème \ref{eq:M}.
\end{corollaire}

\section{Métrique de Wasserstein}

\section{Gaussiennes et Transport Optimal, Dualité}

\section{Pas encore de contenu}

\section{Barycentres et Lois Multimarginales}

\section{Barycentre de Sinkhorn}

\section{Modèles de Flot et Diffusion}

\end{document}
