\documentclass[info, math, french]{mpb-cours}

\title{Transport Optimal Computationnel}
\author{D'après Gabriel Peyré}

\DeclareMathOperator{\Wass}{W}
\DeclareMathOperator{\diag}{diag}
\def\WW{\mathbb{W}}
\def\X{\mathcal{X}}
\def\Y{\mathcal{Y}}

\begin{document}
\bettertitle
\begin{abstract}
	\url{mailto:gabriel.peyre@ens.fr}
	Notes de cours sur \url{https://arxiv.org/abs/2505.06589}
	Syllabus: \url{https://docs.google.com/document/u/0/d/1JlDpcS0tkzX8CSgHlUf13ZHQRWu650EtNLrycT39dxk/mobilebasic}
\end{abstract}

\section*{Introduction}
L'une des motivations principales du cours est de comparer, en apprentissage statistique,
des données sous formes de distributions de probabilités, souvent discrètes (nuages de points).
On peut penser au transport optimal comme de l'apprentissage non-supervisé: comment associer une
distribution de probabilité paramétrique $\alpha_{\theta}$ à une probabilité observée $\beta$ sur un groupe de points.
Dans ce cours, les lettres grecques sont réservées aux distributions de probabilités et les lettres latines
aux points.
Pour ce faire, on va faire une association de densité $\min_{\theta} D(\alpha_{\theta}, \beta)$ qui
prend en compte une métrique $d$.
L'idée étant d'utiliser la métrique $d$ pour définir la métrique $D$, en utilisant la structure
de l'espace sous-jacent pour les données.
Il faut voir le transport optimal comme un mécanisme d'élévation de l'espace des données vers un espace
de probabilité, de sorte que $D = d$ lorsqu'on considère des diracs.


\input{chapters/monge}

\input{chapters/kantorovitch}

\input{chapters/wasserstein}

\input{chapters/duality}

\input{chapters/sliced}

\input{chapters/diffusion}
\section{Barycentres et Lois Multimarginales}

\input{chapters/sinkhorn}

\end{document}
