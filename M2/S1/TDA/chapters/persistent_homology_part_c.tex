\subsection{Persistence et apprentissage automatique}
\subsubsection{Représentation de persistence}
Puisque l'espace des diagrammes de persistence n'est pas linéaire, les algorithmes de ML classique ne
fonctionnent pas bien.
La bibliothèque Python et C++ \emph{Gudhi} propose une large zoologie de représentations pour la persistence,
comme mesures discrètes, espaces métriques finis, racines de polynômes ou collections de fonctions 1D.

Par exemple, on peut représenter un diagramme en le plongeant dans $\R^{2}$ et l'espace des mesures par $D = \sum \delta_{p_{i}}$.
Si on se donne un noyau $K: \R^{2} \to \R$ et $H$ une matrice de bande-passante (forme quadratique), en définissant $K_{H}(u) = \abs{H}^{-1/2}K(H^{-1/2}\cdot u)$, on obtient alors, étant donné une fonction de poids $w$, \define{la surface de persistence} de $D$ par:
\begin{equation*}
	\forall u \in \R^{2}, \rho(D)(u) = D(wK_{H}(u - \cdot))
\end{equation*}

La question se pose alors de savoir comment choisir une représentation adaptée à un réseau de neurones.
Une réponse partielle peut être trouvée en regardant l'architecture à ensembles profonds:
on se donne $n$ points dans $\R^{d}$ et on construit un réseau dont les niveaux sont invariants
par permutation ($f \circ \sigma = f$)
\begin{thm}[Universalité]
	Une fonction $f$ est invariante par permutation si et seulement si, pour tout $X$ inclus dans un ensemble dénombrable $f(X) = \rho(\sum_{i} \phi(x_{i}))$ pour certaines fonctions $\rho$ et $\phi$.
\end{thm}

Les réseaux à niveaux invariants par permutation permettent de généraliser plusieurs approches générales en TDA, sous la forme:
\begin{equation*}
	\mathrm{PersLay}(\diag) = \rho(\mathrm{op}\{w(p), \phi(p)\}_{p\in \diag})
\end{equation*}
où $\mathrm{op}$ est invariante par permutation, $w$ est une fonction de poids, et $\phi$ est une transformation permettant de se ramener à un ensemble dénombrable.

On peut par exemple retrouver la surface de persistence en se donnant $t_{1}, \ldots, t_{q}\in \R^{2}$ puis en posant:
\begin{itemize}
	\item $w(p) = w_{t}((x, y))$;
	\item $\phi_{\Gamma}: p \mapsto (\Gamma_{p}(t_{i}))_{i}$ avec $\Gamma_{p}$ la gaussienne centrée en $p$ d'écart-type fixé $\sigma$;
	\item $\mathrm{op} = \sum$.
\end{itemize}
Pour les paysages, on prend $w(p) = 1$, $\mathrm{op} = \mathrm{top-}k$ et $\phi_{\Lambda}$ l'évaluation de $\Lambda_{p}$ en $q$ paramètres $t_{1}, \ldots, t_{q}$.

\subsubsection{Différentiabilité de la persistence}
Nombre de méthodes permettent de minimiser une fonction sur l'ensemble des diagrammes, mais la plupart sont
restreintes à un type spécifique de filtration ou de fonction à minimiser.
