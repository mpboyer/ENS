\documentclass[math, info]{cours}
\title{Homework 1\\[-5pt] {\small Convex Optimization}}
\author{Matthieu Boyer}

\begin{document}
\maketitle

\section{Exercises}
\subsection*{Convex Sets}
\begin{itemize}
	\item Let $\mC = \left\{ x \in \R_{+}^{2} \mid x_{1}x_{2} \geq 1 \right\}$.
		We can see that $\mC = \left\{ (x_{1}, x_{2}) \in \R_{+*}^{2} \mid x_{1} \geq \frac{1}{x_{2}} \right\}$.
		Thus, $\mC$ is the epigraph of $x \mapsto \frac{1}{x}$ which is convex since its second derivative is $x \mapsto \frac{2x}{x^{4}}$.
		Thus, $\boxed{\mC \text{ is convex.}}$
	\item In general, $\boxed{\text{this set is not convex.}}$ Indeed, for sets in $\R$, take $S = \{-1, 1\}, T = \{0\}$.
		Then the set of points closer to $S$ than to $T$ is $\{x \in \R \mid x\leq -0.5 \lor x \geq 0.5\}$ which is not an interval and thus not convex.
			\item Let us take $x_{1}, x_{2} \in \left\{ x \mid x + S_{2} \subseteq S_{1} \right\}$.
		For $x = \lambda x_{1} + \left( 1 - \lambda \right)x_{2}$ consider $x + y$ for $y \in S_{2}$:
		\begin{equation*}
			x + y = \lambda x_{1} + \left(1 - \lambda \right)x_{2} + y = \lambda(x_{1} + y) + (1 - \lambda x_{2} + y)
		\end{equation*}
		Since $x_{i} + S_{2}\subseteq S_{1}$ for $i = 1, 2$, and since $S_{1}$ is convex, for all $y \in S_{2}$ the above sum is in $S_{1}$ and thus $x + S_{2} \subseteq S_{2}$.
		Finally, \boxed{\text{our set is convex.}}
	\item Let $x_{1}, x_{2} \in \left\{ x \mid \exists y \in S_{2}, x + y \in S_{1} \right\}$.
		Then let $y_{1}, y_{2}$ be associated points to $x_{1}, x_{2}$.
		For $x = \lambda x_{1} + \left( 1 - \lambda \right)x_{2}$ and $y = \lambda y_{1} + (1 - \lambda) y_{2}$.
		Then, since $S_{2}$ is convex, $y \in S_{2}$.
		Moreover:
		\begin{equation*}
			x + y = \underbrace{\lambda \underbrace{(x_{1} + y_{1})}_{\in S_{1}} + \left( 1 - \lambda \right)\underbrace{(x_{2} + y_{2})}_{\in S_{1}}}_{\in S_{1} \text{ since } S_{1} \text{ is convex}}
		\end{equation*}
		Then, \boxed{\text{our set is convex.}}
\end{itemize}

\subsection*{Convex Functions}
\begin{itemize}
	\item The hessian of $f$ at $(x,y)$ is the matrix $\begin{pmatrix}
			0 & 1\\
			1 & 0
		\end{pmatrix}$.
		On $\R^{2}$ this matrix is not positive nor negative and thus the function is neither convex, or concave.
However, the upper level sets for $\alpha$ are convex if and only if $\alpha \geq 0$ (from our first example) and thus $\boxed{\text{the function is neither quasi-concave nor quasi-convex.}}$
	\item On $\R_{+*}^{2}$, the hessian of the function is positive semidefinite and thus $\boxed{\text{the function is convex.}}$
	\item The hessian matrix of $f$ is the matrix$\begin{pmatrix}
			0 & -\frac{1}{x_{2}^{2}}\\
			-\frac{1}{x_{2}^{2}} & \frac{2x_{1}}{x_{2}^{3}}
		\end{pmatrix}$
		Since its determinant is $<0$ the Hessian is not positive semi-definite and the function is not convex.
		However, its sublevel sets are defined by the equations $x_{1} \leq \alpha x_{2}$ and are thus convex (since half-planes).
		Thus, $\boxed{f \text{ is quasi-convex}}$.
	\item Let us define the LÃ¶wner order $\preceq$ as the partial order defined by the convex cone of positive semi-definite matrices.
		We know that:
		\begin{equation*}
			A \preceq B \Rightarrow B^{-1} \preceq A^{-1}, \forall A, B \in S_{++}^{n}
		\end{equation*}
		From this, we know that for all $t \leq 1$:
		\begin{equation*}
			\left( \left( 1 - t \right) X + tY \right)^{-1} \preceq \left( 1 - t \right)X^{-1} + tY^{-1}
		\end{equation*}
		By linearity of the trace, we can now see that $\boxed{X \mapsto \Tr(X^{-1}) \text{ is convex}}$.
\end{itemize}

\subsection*{Fenchel Conjugate}
\begin{itemize}
	\item We have:
		\begin{equation*}
			\boxed{%
			f^{*}(y) = \sup_{x}\left( \transpose{x}y -\norm{x} \right) = \begin{cases}
				0 & \text{if } \norm{y}_{*} \leq 1\\
				\infty &  \text{otherwise}
			\end{cases}}
		\end{equation*}
		To show this, let $y$ such that $\norm{y}_{*} = \sup_{\norm{x} \leq 1} \norm{\transpose{x}y} \leq 1$.
		Then by Cauchy-Schwarz inequality, we know that $\transpose{x}y \leq \norm{x}\norm{y}_{*}\leq \norm{x}$ and the term in the supremum is always $\leq 0$.
		If $\norm{y}_{*} > 1$ however, then there exists $z$ such that $\norm{z} \leq 1$ such that $\transpose{z}y > 1$.
		Taking $x = tz$ in the supremum, we know that $f^{*}(y) \geq t\left( \transpose{z}y -\norm{z} \right)$ which goes to infinity with $t \to \infty$.
		Thus, the fenchel conjugate of a norm is the convex indicator of the unit ball of the dual norm.
	\item Let us denote the infimal convolution of $g, h$ by $g \square h$.
		Then we will show that:
		\begin{equation*}
			\boxed{%
			\left( g \square h \right)^{*} = \left( g^{*} + h^{*} \right)}
		\end{equation*}
		Note that:
		\begin{equation*}
			\begin{aligned}
				\left( g\square h \right)^{*}(\alpha) =& \sup_{x, y} \left\{ \transpose{x}\alpha - g(y) - h(x - y) \right\}\\
			=& \sup_{x_{1}, x_{2}} \left\{ \transpose{\left( x_{1} + x_{2} \right)}\alpha - g(x_{1}) - h(x_{2}) \right\}\\
			=& \sup_{x_{1}, x_{2}} \left\{ \transpose{x_{1}}\alpha - g(x_{1})\right\} + \sup_{x_{2}}\left\{ \transpose{x_{2}}\alpha - h(x_{2}) \right\}\\
			=& g^{*} + h^{*}
		\end{aligned}
		\end{equation*}
		Given $g = \norm{\cdot}_{1}$ and $h = \frac{1}{2\alpha}\norm{\cdot}_{2}^{2}$.
		Let $x = u + v$. Then:
		\begin{equation*}
			f(x) = \inf_{v} \left\{ g(x - v) + h(v) \right\}
		\end{equation*}
		Substituting the expressions, we get:
		\begin{equation*}
			\begin{aligned}
				f(x) =& \inf_{v}\left\{ \norm{x - v}_{1} + \frac{1}{2\alpha}\norm{v}_{2}^{2} \right\}\\
				=& \sum_{i = 1}^{n} \inf_{v_{i}} \left\{ \abs{x_{i} - v_{i}} + \frac{1}{2\alpha}v_{i}^{2} \right\}
			\end{aligned}
		\end{equation*}
		We will now compute the infima independently. We have two cases:
		\begin{enumerate}
			\item $v_{i} \leq x_{i}$: Let $\phi(v_{i}) = x_{i} - v_{i} + \frac{1}{2\alpha}v_{i}^{2}$. We want to find a minimum for $\phi$, which is found at $v_{i} = \alpha$. This solution is valid if $\alpha \leq x_{i}$.
			\item $v_{i} > x_{i}$: Let $\phi(v_{i} = \frac{1}{2\alpha} + v_{i} - x_{i}$. We want to find a minimum for $\phi$, which is at $v_{i} = -\alpha$. This solution is valid if $\alpha > x_{i}$.
		\end{enumerate}
		Then, we have three possible cases for the optimal $v_{i}$:
		\begin{enumerate}
			\item If $x_{i} \geq \alpha$, $v_{i} = \alpha$
			\item If $x_{i} \leq -\alpha$, $v_{i} = -\alpha$
			\item If $-\alpha < x_{i} < \alpha$, $v_{i} = x_{i}$.
		\end{enumerate}
		Plugging this into $f$:
		\begin{equation*}
			\boxed{%
			f(x) = \sum_{i = 1}^{n} \left( \begin{cases}
					\frac{1}{2\alpha}x_{i}^{2} & \abs{x_{i}}\leq \alpha\\
					\abs{x_{i} - \alpha} - \frac{\alpha}{2} & \abs{x_{i}} > \alpha
		\end{cases}\right)}
		\end{equation*}
		Now, clearly, $f = g\square h$ is a convex function (as the infimum of two convex functions (if $\alpha > 0$)).
		Moreover, $f$ is clearly lower semi-continuous from its expression.
		Now, we have $\boxed{f^{**} = f}$ from the Fenchel-Moreau theorem (proved below):
		\begin{thm}
			The biconjugate of $f$ is the largest lower semi-continuous convex function below than $f$.
		\end{thm}
		\begin{proof}
			Let $x \in \R^{n}$. For all $y$, the directional derivative:
			\begin{equation*}
				\partial_{y}f(x) = \lim_{\d t \to 0} \frac{f\left( x + \d t y \right) - f(x)}{\d t}
			\end{equation*}
			is a sublinear as a function of $y$.
			From the Hahn-Banach theorem, there exists $\tilde{\partial f} \in \left(\R^{n}\right)^{*} = \R^{n}$ such that:
			\begin{equation*}
				\scalar{\tilde{\partial f},\cdot }\leq \partial_{y}f(x) \leq f\left( x + y \right) - f(x), \forall y \in \R^{n}
			\end{equation*}
			Then:
			\begin{equation*}
				f(x) + f^{*}\left( \tilde{\partial f} \right) = \scalar{\tilde{\partial f}, x}
			\end{equation*}
			which completes our proof that $f^{**} = f$.
		\end{proof}

	\item We want to compute:
		\begin{equation*}
			\ell^{*}(y) = \sup_{z\in \R}\left\{ yz - \ell(z) \right\} \text{ where } \ell: z\mapsto \log\left( 1 + e^{z} \right)
		\end{equation*}
		We differentiate $\phi(z)$ the function in the supremum:
		\begin{equation*}
			\phi'(z) = y - \frac{1}{1 + e^{z}}
		\end{equation*}
		Then, we find $z = \log\left( \frac{1 - y}{y} \right)$, which only makes sense for $y \in ]0, 1[$.
		Finally, we compute:
		\begin{equation*}
			\begin{aligned}
				\ell^{*}(y) =& y\log\left( \frac{1-y}{y} \right) + \log(y)\\
				=& y\log(1 - y) - y\log(y) + \log(y)\\
				=& y\log\left( 1 - y \right) + \left( 1 - y \right)\log(y)
			\end{aligned}
		\end{equation*}
		In the end,
		\begin{equation*}
			\boxed{y\log\left( 1 - y \right) + \left( 1 - y \right)\log\left( y \right)}
		\end{equation*}
\end{itemize}

\subsection*{Duality}
\begin{itemize}
	\item The Lagrangian of the problem associated with $Ax = b$ is:
		\begin{equation*}
			\mL(x, \lambda) = \frac{1}{2}\norm{Ax - b}_{2}^{2} + \alpha\norm{x}_{1} - \transpose{\lambda}\left( A x - b \right) = \frac{1}{2}\norm{Ax -b}_{2}^{2} + \alpha\norm{x}_{1} - \transpose{\lambda}Ax + \transpose{\lambda}b
		\end{equation*}
		Then, by separating terms involving $x$:
		\begin{equation*}
			\mL(x, \lambda) = \frac{1}{2}\norm{b}_{2}^{2} - \frac{1}{2}\norm{\lambda - b}^{2}_{2} + \transpose{\lambda}b + \alpha\norm{x}_{1} - \left( \transpose{\lambda}A \right)x
		\end{equation*}
		In solving $\displaystyle\max_{\lambda} \min_{x}\mL(x, \lambda)$, the inner minimization on $x$ only depends on $\norm{x}_{1}$ and $\left( \transpose{\lambda}A \right)x$.
		Minimizing it results in $\norm{\transpose{A}\lambda}_{\infty} \leq \alpha$.
		Finally, we get that:
		\begin{equation*}
			\boxed{\max_{\lambda \in \R^{m}} -\frac{1}{2}\norm{\lambda - b}_{2}^{2} + \frac{1}{2}\norm{b}_{2}^{2} - \inf_{\left\{ \norm{\cdot}_{\infty} \leq 1 \right\}}\left( \frac{\transpose{A}\lambda}{\alpha} \right) \text{ is a dual problem for the LASSO}}
		\end{equation*}
	\item We start with:
		\begin{equation*}
			\min_{w, w_{1}, \ldots, w_{m}\in\R^{n}} \left\{ \frac{1}{n}\sum_{i = 1}^{m}h_{i}\left( w_{i} \right) + \frac{\lambda}{2}\norm{w}_{2}^{2}\ \middle|\ w_{i} = w, \forall i \in \onen{m} \right\}
		\end{equation*}
		Its Lagrangian is:
		\begin{equation*}
			\mL\left( w, w_{1}, \ldots, w_{m}, v_{1}, \ldots, v_{m} \right) = \frac{1}{n}\sum_{i = 1}^{m} h_{i}\left( w_{i} \right) + \frac{\lambda}{2}\norm{w}_{2}^{2} + \sum_{i = 1}^{m}\transpose{v_{i}}\left( w - w_{i} \right) = \frac{\lambda}{2}\norm{w}_{2}^{2} + \sum_{i = 1}^{m}\transpose{v_{i}}w + \sum_{i = 1}^{m}\frac{1}{n}h_{i}\left( w_{i} \right) - \transpose{v_{i}}w_{i}
		\end{equation*}
		For the minimization step, we minimize for each of the $w_{i}$ and for $w$.
		Let $g_{i}\left( v_{i} \right)$ be the minimized value of $\frac{1}{n}h_{i}\left( w_{i} \right) - \transpose{v_{i}}w_{i}$.
		For $w$, we want to minimize $\frac{\lambda}{2}\norm{w}_{2}^{2} + \sum_{i = 1}^{m}\transpose{v_{i}}w$,
		which is quadratic in $w$.
		Computing its minimum we find $\frac{-1}{2\lambda}\norm{\sum_{i = 1}^{m}v_{i}}_{2}^{2}$.
		Finally, the dual problem is:
		\begin{equation*}
			\boxed{\max_{v_{1},\ldots, v_{m}}\sum_{i = 1}^{m}g_{i}(v_{i}) - \frac{1}{2\lambda}\norm{\sum_{i = 1}^{m}v_{i}}_{2}^{2}}
		\end{equation*}
		For logistic regression, we have:
		\begin{equation*}
			h_{i}: w\mapsto \log\left( 1 + \exp \left( -y_{i}\transpose{x_{i}}w \right) \right)
		\end{equation*}
		Then we have:
		\begin{equation*}
			g_{i}\left( v_{i} \right) = \min_{w_{i}} \left\{ \frac{1}{n}\log\left( 1 + \exp\left( -y_{i}\transpose{x_{i}}w_{i} \right) \right) - \transpose{v_{i}}w_{i} \right\}
		\end{equation*}
		Since $h_{i}$ is differentiable, we can compute its gradient:
		\begin{equation*}
			\nabla h_{i}\left( w_{i} \right) = \frac{1}{n}\cdot \frac{-y_{i}x_{i}}{1 + \exp\left( y_{i}\transpose{x_{i}}w_{i} \right)}
		\end{equation*}
		which gives us a minimality condition:
		\begin{equation*}
			\frac{-y_{i}x_{i}}{1 + \exp\left( y_{i}\transpose{x_{i}}w_{i} \right)} = nv_{i}
		\end{equation*}
		which rearranges to:
		\begin{equation*}
		w_{i} = \frac{
			-y_{i}\log\left(
				\frac{-y_{i}\transpose{v_{i}}x_{i} - n\norm{v_{i}}_{2}^{2}}
				     {n\norm{v_{i}}_{2}^{2}}
	     		\right)}
		{
			\norm{x_{i}}_{2}^{2}
		}x_{i} = \frac{-y_{i}t}{\norm{x_{i}}_{2}^{2}}x_{i}
		\end{equation*}
		Inputing this into $y_{i}\transpose{x_{i}}w_{i}$:
		\begin{equation*}
			y_{i}\transpose{x_{i}}w_{i} = +\underbrace{y_{i}^{2}}_{=1} \overbrace{\frac{\transpose{x_{i}}x_{i}}{\norm{x_{i}}_{2}^{2}}}^{=1} t = t
		\end{equation*}
		Thus:
		\begin{equation*}
			g_{i}(w_{i}) = \frac{1}{n} \log\left( -\frac{y_{i}\transpose{v_{i}}x_{i}}{\norm{v_{i}}_{2}^{2}} \right)- \frac{y_{i}\transpose{v_{i}}w_{i}}{\norm{x_{i}}_{2}^{2}}\log\left( -1 - \frac{y_{i}\transpose{v_{i}}x_{i}}{\norm{v_{i}}_{2}^{2}} \right)
		\end{equation*}
		Finally our dual problem is:
		\begin{equation*}
			\boxed{\max_{v_{1}, \ldots, v_{m}}\sum_{i = 1}^{m}\frac{1}{n} \log\left( -\frac{y_{i}\transpose{v_{i}}x_{i}}{\norm{v_{i}}_{2}^{2}} \right)- \frac{y_{i}\transpose{v_{i}}w_{i}}{\norm{x_{i}}_{2}^{2}}\log\left( -1 - \frac{y_{i}\transpose{v_{i}}x_{i}}{\norm{v_{i}}_{2}^{2}} \right) - \frac{1}{2\lambda} \norm{\sum_{i = 1}^{m}v_{i}}_{2}^{2}}
		\end{equation*}
		and I will not be trying to find a closer form manually.
	\item We consider the problem:
		\begin{equation*}
			\min_{X\in \S^{n}} \left\{ \Tr\left( A_{0}X \right)\ \middle|\ X \succeq 0, \Tr\left( A_{1}X \right) = b_{1}, \ldots, \Tr\left( A_{m}X \right) = b_{m} \right\}
		\end{equation*}
		Its Lagrangian is (for $\lambda_{i} > 0, S \succeq 0$):
		\begin{equation*}
			\mL\left( X, \lambda_{1}, \ldots, \lambda_{m}, S\right) = \Tr\left( A_{0}X \right) + \sum_{i = 1}^{m}\lambda_{i}\left( \Tr\left( A_{i}X \right) - b_{i} \right) + \Tr\left( \transpose{S}X \right) = \Tr\left( \left( A_{0} + \sum_{i = 1}^{m} \lambda_{i}A_{i} + S \right)X \right) - \sum_{i = 1}^{m}\lambda_{i}b_{i}
		\end{equation*}
		Minimizing everything in $X$ can be done since everything is differentiable in $X$:
		\begin{equation*}
			\nabla \mL = \transpose{A_{0}} + \sum_{i = 1}^{m}\lambda_{i}\transpose{A_{i}} + 2 SX = A_{0} + \sum_{i = 1}^{m}\lambda_{i}A_{i} + S
		\end{equation*}
		which is $0$ when:
		\begin{equation*}
			S = -\left( A_{0} + \sum_{i = 1}^{m} \lambda_{i}A_{i}\right)
		\end{equation*}
		Finally we get the dual problem:
		\begin{equation*}
			\boxed{\max_{\lambda_{1},\ldots, \lambda_{m}, S} \left\{ -\sum_{i = 1}^{m}\lambda_{i}b_{i}\ \middle| \ S \succeq 0, S = -A_{0} - \sum_{i = 1}^{m}\lambda_{i}A_{i} \right\}}
		\end{equation*}
		Strong duality holds if, and only if, the Karush-Kuhn-Tucker conditions are satisfied.
		Since $\Tr\left( S_{*}X_{*} \right)$ is a rewriting of complementary slackness, clearly strong duality implies $\Tr\left(  S_{*}X_{*} \right) = 0$.
		If we have $\Tr\left( S_{*}X_{*} \right) = 0$ for the optimal solutions, in particular, both the primal and dual problems are attained, the gradient computed earlier vanishes and thus the KKT conditions are satisfied.
		Finally:
		\begin{equation*}
			\boxed{\text{Strong Duality holds if and only if} \Tr\left( X_{*}S_{*} \right) = 0}
		\end{equation*}
\end{itemize}

\section{Problem}
We consider the following problem:
\begin{equation}
	\min_{w \in \R^{n}} \frac{1}{2}\norm{w}_{2}^{2} + \alpha\sum_{i = 1}^{m} \max\left\{ 0, 1 - \transpose{w}x_{i}y_{i} \right\}
	\label{SVM}
\end{equation}
\subsection{Dual problem.}
Considering the constraints, we have:
\begin{equation*}
	s_{i} \geq 1 - y_{i}\transpose{w}x_{i} \land s_{i} \geq 0
\end{equation*}
and thus, minimizing in $s$, for $w$ fixed gives:
\begin{equation*}
	s_{i} = \max \left\{ 0, 1 - y_{i}\transpose{w}x_{i} \right\}
\end{equation*}
which proves the equivalent reformulation:
\begin{equation*}
	\begin{aligned}
	\min_{w\in \R^{n}, s\in \R^{m}} \frac{1}{2}\norm{w}_{2}^{2} + \alpha\sum_{i = 1}^{m} s_{i}&\\
	\text{s. t. } \transpose{w}x_{i}y_{i} \geq 1 - s_{i}&\\
	s_{i}\geq 0&
	\end{aligned}
\end{equation*}

Computing the dual problem needs two constraints:
\begin{equation*}
	\lambda_{i} \geq 0 \text{ for } \transpose{w}X_{i}\geq 1 - s_{i} \text{ and } \mu_{i} \geq 0 \text{ for } s_{i} \geq 0
\end{equation*}
The Lagrangian is thus:
\begin{equation*}
	\mL\left( w, s, \lambda, \mu \right) = \frac{1}{2}\norm{w}_{2}^{2} + \alpha\sum_{i = 1}^{m}s_{i} - \sum_{i = 1}^{m}\lambda_{i}\left( \transpose{w}X_{i} - 1 + s_{i} \right) - \sum_{i = 1}^{m}\mu_{i}s_{i}
\end{equation*}
Then:
\begin{align*}
	\frac{\partial \mL}{\partial w} = w - \sum_{i = 1}^{m}\lambda_{i}X_{i} = 0 \Rightarrow w = X\lambda\\
	\frac{\partial \mL}{\partial s_{i}} = \alpha - \lambda_{i} - \mu_{i} = 0 \Rightarrow \lambda_{i} \leq \alpha
\end{align*}
Finally, subtituting, the dual problem is:
\begin{equation*}
	\max_{0 \leq \lambda \leq \alpha} \mD\left( \lambda \right) = -\frac{1}{2}\transpose{\lambda}\transpose{X}X\lambda + \sum_{i = 1}^{m}\lambda_{i}
\end{equation*}
\subsection{Algorithms}
\begin{itemize}
	\item
\end{itemize}<++>

\end{document}
